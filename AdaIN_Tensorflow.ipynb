{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AdaIN_Tensorflow.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMcMmeyLSrQWzQtRhaAQsE1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaxHuerlimann/AdaIN-Style-tf2/blob/master/AdaIN_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48GCj08y88Np",
        "colab_type": "text"
      },
      "source": [
        "Tensorflow implementation of *AdaIN-Style* network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfmofVc4iYOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install torchfile to extract torch model\n",
        "!git clone https://github.com/bshillingford/python-torchfile.git\n",
        "%cd python-torchfile/\n",
        "!python setup.py install\n",
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J25t_3hViFgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import datetime\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchfile\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzynkWpYLrGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from tensorflow.python.keras.preprocessing import image as kp_image\n",
        "from tensorflow.python.keras import models \n",
        "from tensorflow.python.keras import losses\n",
        "from tensorflow.python.keras import layers\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH_-lGwbET2l",
        "colab_type": "text"
      },
      "source": [
        "# Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fert1e8h_Nr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
        "summary_writer = tf.summary.create_file_writer(train_log_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9PsXXlers8V",
        "colab_type": "text"
      },
      "source": [
        "Setup Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLHSBPaZT809",
        "colab_type": "text"
      },
      "source": [
        "Get kaggle configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rtVq6l8r5HB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!pip3 install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/gdrive/'My Drive'/Kaggle/kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json  # set permission\n",
        "drive.flush_and_unmount()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlgYX7oKT_YA",
        "colab_type": "text"
      },
      "source": [
        "Download the data from kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP4eCpe8mUOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_dir = Path('/content/data/train_images')\n",
        "data_package = 'train_1'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y7_o0ZRpN6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(img_dir):\n",
        "  os.makedirs(img_dir)\n",
        "!kaggle competitions download painter-by-numbers -f {data_package}.zip -p {img_dir.as_posix()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNVHZLMp8v-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q {(img_dir / data_package).as_posix()} -d {img_dir.as_posix()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-j8rfcBxnND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm {img_dir.as_posix()}/*.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY78eGILsrpw",
        "colab_type": "text"
      },
      "source": [
        "Download MSCOCO dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pzr05dTsx35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -p {(img_dir / 'ms_coco.zip').as_posix()} http://images.cocodataset.org/zips/train2017.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA2RQkaKAnid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv /content/images.cocodataset.org/zips/train2017.zip /content/data/train_images/ms_coco.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXlu36emtOzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q {(img_dir / 'ms_coco').as_posix()} -d {img_dir.as_posix()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yRc8vV2CxqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm {img_dir.as_posix()}/*.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXuA2mA3cd6r",
        "colab_type": "text"
      },
      "source": [
        "Download vgg weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BB_TB_IXgsFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -c https://s3.amazonaws.com/xunhuang-public/adain/vgg_normalised.t7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBsq0ti3MDpo",
        "colab_type": "text"
      },
      "source": [
        "Visualize Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNAox_GPMFjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_img(path_to_img):\n",
        "  max_dim = 512\n",
        "  img = Image.open(path_to_img)\n",
        "  long = min(img.size)\n",
        "  scale = max_dim/long\n",
        "  img = img.resize((round(img.size[0]*scale), round(img.size[1]*scale)), Image.ANTIALIAS)\n",
        "  \n",
        "  img = kp_image.img_to_array(img)\n",
        "  \n",
        "  # We need to broadcast the image array such that it has a batch dimension \n",
        "  img = np.expand_dims(img, axis=0)\n",
        "  return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68E71m1zMK0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  def imshow(img, title=None):\n",
        "    # Remove the batch dimension\n",
        "    out = np.squeeze(img, axis=0)\n",
        "    # Normalize for display \n",
        "    out = out.astype('uint8')\n",
        "    plt.imshow(out)\n",
        "    if title is not None:\n",
        "      plt.title(title)\n",
        "    plt.imshow(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsswzpiBPknV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content_path = img_dir / 'train2017' / '000000000009.jpg'\n",
        "style_path = img_dir / data_package / '1007.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0zAnhxLMVct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "content = load_img(content_path.as_posix()).astype('uint8')\n",
        "style = load_img(style_path.as_posix()).astype('uint8')\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "imshow(content, 'Content Image')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "imshow(style, 'Style Image')\n",
        "plt.show()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_V9IWhBLiRR",
        "colab_type": "text"
      },
      "source": [
        "Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBRyopH8MVra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clean datasets\n",
        "corrupted_files = []\n",
        "def check_jpegs(path_to_img_dir):\n",
        "  \"\"\"Check if the jpegs can be loaded, otherwise list names.\"\"\"\n",
        "  for f in path_to_img_dir.glob('*.jpg'):\n",
        "    print(f)\n",
        "    data = cv2.imread(f.as_posix())\n",
        "    print(data.size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90BWrpzhOdA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check_jpegs((img_dir / 'train2017'))\n",
        "# check_jpegs((img_dir / data_package))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlWE0YDOb3Rd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Workaround for keras application bug only being able to be initialized once\n",
        "# Add before any TF calls\n",
        "# Initialize the keras global outside of any tf.functions\n",
        "temp = tf.random.uniform([4, 32, 32, 3])  # Or tf.zeros\n",
        "tf.keras.applications.vgg16.preprocess_input(temp)\n",
        "print(\"Noice\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYCl3lCASmR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_and_process_img(path_to_img):\n",
        "  \"\"\"Load the image and preprocess according to trained VGG19 model standards.\n",
        "  \"\"\"\n",
        "  new_min_dim = 512\n",
        "  img = tf.io.read_file(path_to_img)\n",
        "  # This creates RGB image\n",
        "  try:\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "  except:\n",
        "    return None\n",
        "\n",
        "  # Scale minimum dimension to 512px\n",
        "  height = tf.cast(tf.shape(img)[0], tf.float32)\n",
        "  width = tf.cast(tf.shape(img)[1], tf.float32)\n",
        "  min_dim = tf.minimum(height, width)\n",
        "  scale = new_min_dim / min_dim\n",
        "  img = tf.image.resize(img, (scale*height, scale*width))\n",
        "\n",
        "  # This scales pixel values and reorders channels to BGR\n",
        "  #img = tf.keras.applications.vgg19.preprocess_input(img)\n",
        "  # img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "  # img = tf.cast(img, tf.float32)\n",
        "  img /= 255.\n",
        "\n",
        "  return img\n",
        "\n",
        "\n",
        "def preprocess_img(img):\n",
        "  \"\"\"Preprocess image.\"\"\"\n",
        "  crop_size = 256\n",
        "  img = tf.image.random_crop(img, (crop_size, crop_size, 3))\n",
        "\n",
        "  return img\n",
        "\n",
        "\n",
        "def process_path(path_to_img):\n",
        "  img = load_and_process_img(path_to_img)\n",
        "  img = preprocess_img(img)\n",
        "  return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQkTojNkWKNT",
        "colab_type": "text"
      },
      "source": [
        "Create tensorflow Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UZN9NfesIRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "def prepare_dataset(data):\n",
        "  data = data.map(process_path)\n",
        "  # data = data.repeat()\n",
        "  data = data.batch(BATCH_SIZE)\n",
        "  data = data.prefetch(AUTOTUNE)\n",
        "\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrZi3tEwQVQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get number of train data\n",
        "NUM_STYLE_IMAGES = len(list((img_dir / data_package).glob('*.jpg')))\n",
        "NUM_CONTENT_IMAGES = len(list((img_dir / 'train2017').glob('*.jpg')))\n",
        "NUM_STYLE_BATCHES = math.ceil(NUM_STYLE_IMAGES / BATCH_SIZE)\n",
        "NUM_CONTENT_BATCHES = math.ceil(NUM_CONTENT_IMAGES / BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7zq4zCBWOEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Style images\n",
        "style_dataset = tf.data.Dataset.list_files(str(img_dir / data_package / '*.jpg'))\n",
        "style_dataset = prepare_dataset(style_dataset)\n",
        "# Content images\n",
        "content_dataset = tf.data.Dataset.list_files(str(img_dir / 'train2017' / '*.jpg'))\n",
        "content_dataset = prepare_dataset(content_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JLp6NNRLkyI",
        "colab_type": "text"
      },
      "source": [
        "Model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKuAU60iLbM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Content layer where will pull our feature maps\n",
        "content_layers = ['conv4_1'] \n",
        "\n",
        "# Style layer we are interested in\n",
        "style_layers = ['conv1_1',\n",
        "                'conv2_1',\n",
        "                'conv3_1', \n",
        "                'conv4_1' \n",
        "               ]\n",
        "\n",
        "num_content_layers = len(content_layers)\n",
        "num_style_layers = len(style_layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXS0Z_Ssisgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import torch model into tensorflow\n",
        "def get_encoder_from_torch(target_layer='relu4_1'):\n",
        "  \"\"\"Load a model from t7 and translate it to tensorflow.\"\"\"\n",
        "  t7 = torchfile.load('/content/vgg_normalised.t7', force_8bytes_long=True)\n",
        "\n",
        "  inputs = tf.keras.Input((None, None, 3), name=\"vgg_input\")\n",
        "\n",
        "  x = inputs\n",
        "    \n",
        "  style_outputs = []\n",
        "  content_outputs = []\n",
        "  for idx,module in enumerate(t7.modules):\n",
        "    name = module.name.decode() if module.name is not None else None\n",
        "    \n",
        "    if idx == 0:\n",
        "      name = 'preprocess'  # VGG 1st layer preprocesses with a 1x1 conv to multiply by 255 and subtract BGR mean as bias\n",
        "\n",
        "    if module._typename == b'nn.SpatialReflectionPadding':\n",
        "      x = tf.keras.layers.Lambda(\n",
        "          lambda t: tf.pad(t, [[0, 0], [1, 1], [1, 1], [0, 0]],\n",
        "          mode='REFLECT'))(x)            \n",
        "    elif module._typename == b'nn.SpatialConvolution':\n",
        "      filters = module.nOutputPlane\n",
        "      kernel_size = module.kH\n",
        "      weight = module.weight.transpose([2,3,1,0])\n",
        "      bias = module.bias\n",
        "      x = layers.Conv2D(filters, kernel_size, padding='valid', activation='relu', name=name,\n",
        "                    kernel_initializer=tf.constant_initializer(weight),\n",
        "                    bias_initializer=tf.constant_initializer(bias),\n",
        "                    trainable=False)(x)\n",
        "      if name in style_layers:\n",
        "        style_outputs.append(x)\n",
        "      if name in content_layers:\n",
        "        content_outputs.append(x)\n",
        "    elif module._typename == b'nn.ReLU':\n",
        "      pass # x = layers.Activation('relu', name=name)(x)\n",
        "    elif module._typename == b'nn.SpatialMaxPooling':\n",
        "      x = layers.MaxPooling2D(padding='same', name=name)(x)\n",
        "    else:\n",
        "      raise NotImplementedError(module._typename)\n",
        "\n",
        "    if name == target_layer:\n",
        "      # print(\"Reached target layer\", target_layer)\n",
        "      break\n",
        "  \n",
        "  # Get output layers corresponding to style and content layers \n",
        "  #style_outputs = [vgg.get_layer(name).output for name in style_layers]\n",
        "  #content_outputs = [vgg.get_layer(name).output for name in content_layers]\n",
        "  model_outputs = style_outputs + content_outputs\n",
        "\n",
        "  return models.Model(inputs=inputs, outputs=model_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bfaqUe_6OMPV",
        "colab": {}
      },
      "source": [
        "def get_encoder():\n",
        "  \"\"\" Creates encoder from VGG19 model.\n",
        "  \n",
        "  This function will load the VGG19 model and access the intermediate layers. \n",
        "  These layers will then be used to create a new model that will take input image\n",
        "  and return the outputs from these intermediate layers from the VGG model. \n",
        "  \n",
        "  Returns:\n",
        "    returns a keras model that takes image inputs and outputs the style and \n",
        "      content intermediate layers. \n",
        "  \"\"\"\n",
        "  # Load our model. We load pretrained VGG, trained on imagenet data\n",
        "  vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')\n",
        "  vgg.trainable = False\n",
        "  # Get output layers corresponding to style and content layers \n",
        "  style_outputs = [vgg.get_layer(name).output for name in style_layers]\n",
        "  content_outputs = [vgg.get_layer(name).output for name in content_layers]\n",
        "  model_outputs = style_outputs + content_outputs\n",
        "  # Build model \n",
        "  return models.Model(vgg.input, model_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2KLjB9aYQyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_decoder(encoder):\n",
        "  \"\"\"Creates a trainable decoder, that mirrors the encoder.\n",
        "\n",
        "  Pooling layers are replaced with nearest up-sampling layers and reflection\n",
        "  padding is used to avoid border artifacts.\n",
        "  \"\"\"\n",
        "  decoder = tf.keras.Sequential()\n",
        "  \n",
        "  inputs = tf.keras.Input((None, None, encoder.layers[-1].filters))\n",
        "  # Mirror the encoder\n",
        "  x = inputs\n",
        "  for i in reversed(range(4, len(encoder.layers))):\n",
        "    layer = encoder.layers[i]\n",
        "    if isinstance(layer, layers.MaxPooling2D):\n",
        "      x = layers.UpSampling2D()(x)\n",
        "    elif isinstance(layer, layers.Conv2D):\n",
        "      x = layers.Conv2D(\n",
        "          layer.get_weights()[0].shape[2], \n",
        "          layer.kernel_size, \n",
        "          activation=tf.keras.activations.relu)(\n",
        "              tf.pad(x, [[0, 0], [1, 1], [1, 1], [0, 0]],\n",
        "              mode='REFLECT'))\n",
        "\n",
        "  # Finally reduce number of channels to three\n",
        "  x = tf.pad(x, [[0, 0], [1, 1], [1, 1], [0, 0]],\n",
        "             mode='REFLECT')\n",
        "  x = tf.keras.layers.Conv2D(3, \n",
        "                             3)(x) \n",
        "                             # activation=tf.keras.activations.relu)(x)\n",
        "  outputs = x\n",
        "    \n",
        "  return models.Model(inputs, outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ump31G4rpVxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def adaptive_instance_normalization(x, y):\n",
        "  \"\"\"Aligning the mean and variance of y onto x.\"\"\"\n",
        "  eps = 1e-4\n",
        "  x_mean, x_var = tf.nn.moments(x, [1,2], keepdims=True)\n",
        "  x_std = tf.math.sqrt(x_var)\n",
        "  y_mean, y_var = tf.nn.moments(y, [1,2], keepdims=True)\n",
        "  y_std = tf.math.sqrt(y_var)\n",
        "  # result = y_std * (x - x_mean) / (x_std + eps) + y_mean \n",
        "  result = tf.nn.batch_normalization(x, x_mean, x_var, y_mean, y_std, eps)\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYSs9_DxL5bL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = get_encoder_from_torch()\n",
        "decoder = get_decoder(encoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLLV97MEIDaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(encoder.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qnO1ZlTCDp8A",
        "colab": {}
      },
      "source": [
        "# print(decoder.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIX28vKoLoTi",
        "colab_type": "text"
      },
      "source": [
        "# Define costs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01IEAeHOLW2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_content_loss(adain_output, target_encoded):\n",
        "  return tf.reduce_mean(tf.square(adain_output - target_encoded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MmLAN8fpL1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_style_loss(base_style_encoded, target_encoded):\n",
        "  eps = 1e-5\n",
        "  \n",
        "  base_style_mean, base_style_var = tf.nn.moments(base_style_encoded, \n",
        "                                                  axes=[1,2])\n",
        "  # Add epsilon for numerical stability for gradients close to zero\n",
        "  base_style_std = tf.math.sqrt(base_style_var + eps)\n",
        "\n",
        "  target_mean, target_var = tf.nn.moments(target_encoded,\n",
        "                                          axes=[1,2])\n",
        "  # Add epsilon for numerical stability for gradients close to zero\n",
        "  target_std = tf.math.sqrt(target_var + eps)\n",
        "\n",
        "  mean_diff = tf.reduce_sum(tf.square(base_style_mean - target_mean)) / BATCH_SIZE\n",
        "  std_diff = tf.reduce_sum(tf.square(base_style_std - target_std)) / BATCH_SIZE\n",
        "  return mean_diff + std_diff"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isgX9oZG4NMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "STYLE_LOSS_WEIGHT = 1\n",
        "\n",
        "def get_loss(adain_output, base_style_encoded, target_encoded):\n",
        "  # Content loss\n",
        "  content_loss = get_content_loss(adain_output, target_encoded[-1])\n",
        "  \n",
        "  # Style loss\n",
        "  style_loss = 0\n",
        "  for i in range(num_style_layers):\n",
        "    style_loss += get_style_loss(base_style_encoded[i], target_encoded[i])\n",
        "\n",
        "  return content_loss + STYLE_LOSS_WEIGHT * style_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQFsg3xv_5Dr",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlXByX5ubYfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_img(img, reverse_channels=False):\n",
        "  \"\"\"Decodes preprocessed images.\"\"\"\n",
        "\n",
        "  # perform the inverse of the preprocessiing step\n",
        "  img *= 255.\n",
        "  if reverse_channels:\n",
        "    img = img[..., ::-1]\n",
        "\n",
        "  img = tf.cast(img, dtype=tf.uint8)\n",
        "  return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKrVcxXsBV3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vjqSShX33il",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(content_img, style_img, step):\n",
        "  with tf.GradientTape() as tape:\n",
        "    encoded_content_img = encoder(content_img)\n",
        "    encoded_style_img = encoder(style_img)\n",
        "    tape.watch(encoded_content_img + encoded_style_img)\n",
        "\n",
        "    adain_output = adaptive_instance_normalization(encoded_content_img[-1],\n",
        "                                        encoded_style_img[-1])\n",
        "\n",
        "    target_img = decoder(adain_output)\n",
        "\n",
        "    loss = get_loss(adain_output, encoded_style_img, encoder(target_img))\n",
        "\n",
        "    if step % 50 == 0:\n",
        "      with summary_writer.as_default():\n",
        "        tf.summary.image(\"Target image\", decode_img(\n",
        "            target_img, \n",
        "            reverse_channels=True), \n",
        "            step=step)\n",
        "        tf.summary.scalar(\"Loss\", loss, step=step)\n",
        "\n",
        "  gradients = tape.gradient(loss, decoder.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, decoder.trainable_variables))\n",
        "\n",
        "  train_loss(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ui-8aOo330M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def test_step(content_image, style_image):\n",
        "  encoded_content_img = encoder(content_img)\n",
        "  encoded_style_img = encoder(style_img)\n",
        "  # Only feed the last layer to AdaIN\n",
        "  t = adaptive_instance_normalization(encoded_content_img[-1],\n",
        "                                      encoded_style_img[-1])\n",
        "  target_img = decoder(t)\n",
        "  loss = get_loss(content_img, style_img, target_img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DiDS0-j15q6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 5\n",
        "PROGBAR = tf.keras.utils.Progbar(NUM_STYLE_BATCHES*NUM_CONTENT_BATCHES)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # Reset the metrics at the start of the next epoch\n",
        "  train_loss.reset_states()\n",
        "\n",
        "  step = 0\n",
        "  start_time = time.perf_counter()\n",
        "  for i, content_images in enumerate(content_dataset):\n",
        "    for j, style_images in enumerate(style_dataset):\n",
        "      # Handle loading errors\n",
        "      if content_images is None or style_images is None:\n",
        "        if content_images is None:\n",
        "          tf.print(\"Content image couldn't be loaded.\")\n",
        "        if style_images is None:\n",
        "          tf.print(\"Style image couldn't be loaded.\")\n",
        "        step += 1\n",
        "        PROGBAR.update(step)\n",
        "        break\n",
        "      # print(f\"Image loading: {time.perf_counter() - start_time}\")\n",
        "      # start_time = time.perf_counter()\n",
        "      # Using the file writer, log the reshaped image.\n",
        "      if step % 50 == 0:\n",
        "        with summary_writer.as_default():\n",
        "          tf.summary.image(\"Style data\", decode_img(style_images), step=step)\n",
        "          tf.summary.image(\"Content data\", decode_img(content_images), step=step)\n",
        "      # print(f\"Summary: {time.perf_counter() - start_time}\")\n",
        "\n",
        "      start_time = time.perf_counter()\n",
        "      train_step(content_images, style_images, tf.constant(step, dtype=tf.int64))\n",
        "      print(f\"Train step: {time.perf_counter() - start_time}\")\n",
        "      # start_time = time.perf_counter()\n",
        "      step += 1\n",
        "      PROGBAR.update(step)\n",
        "\n",
        "  template = 'Epoch {}, Loss: {}'\n",
        "  print(template.format(epoch+1,\n",
        "                        train_loss.result()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe0QR1UTLpUp",
        "colab_type": "text"
      },
      "source": [
        "Evalutation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1apW3jODIE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1_1DdqH_zX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}